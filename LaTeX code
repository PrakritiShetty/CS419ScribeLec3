%
% This is the LaTeX template file for lecture notes for CS294-8,
% Computational Biology for Computer Scientists.  When preparing 
% LaTeX notes for this class, please use this template.
%
% To familiarize yourself with this template, the body contains
% some examples of its use.  Look them over.  Then you can
% run LaTeX on this file.  After you have LaTeXed this file then
% you can look over the result either by printing it out with
% dvips or using xdvi.
%
% This template is based on the template for Prof. Sinclair's CS 270.

\documentclass[11pt, twosides]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{cancel} % for \cancel
\usepackage{bbm} % for indicator functions
\usepackage{xcolor}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}
\graphicspath{ {./images/} }

%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
%   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf CS 419M Introduction to Machine Learning
                        \hfill Spring 2021-22} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #3 \hfill Scribe: #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}
}

%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
% Also commands that create a suitable format for the reference list.
% \renewcommand{\cite}[1]{[#1]}
% \def\beginrefs{\begin{list}%
%         {[\arabic{equation}]}{\usecounter{equation}
%          \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
%          \setlength{\labelwidth}{1.6truecm}}}
% \def\endrefs{\end{list}}
% \def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
% \newcommand{\fig}[3]{
% 			\vspace{#2}
% 			\begin{center}
% 			Figure \thelecnum.#1:~#3
% 			\end{center}
% 	}
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
% \newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\begin{document}
%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{3}{Probabilistic estimators}{Prof Abir De}{Team G2}
%\lecture{x}{Title}{Abir De}{Group y}
\section{Topics covered in lecture}
\begin{enumerate}
    \item Surrogate functions in deterministic space
    \item Probabilistic interpretation of training problem
    \item Maximum likelihood Estimator
    \item Approximating function in general situation
\end{enumerate}
\subsection{Shift from Deterministic functions to Probabilistic estimators}
In the last lecture, we saw the approach of defining different deterministic surrogate functions that would potentially help us predict y from x
\begin{enumerate}
    \item Today's lecture began with discussing two such deterministic surrogate functions- \begin{equation} e^{-y \cdot (w^Tx)} \end{equation} and  \begin{equation} \max(1-(w^Tx)y,0)\end{equation}
    \item The latter, also called the \textbf{Hinge Function} has an alternate nomenclature of \begin{equation} (1-(w^Tx)y)_+\end{equation} and this is what is largely known as the \textbf{ReLU function} \begin{equation} \operatorname{ReLU}(1-(w^Tx)y)\end{equation} in the field of ML
    \item Another approach to the same problem would be a probabilistic one, where X and Y are sampled from P, an unknown distribution function 
\end{enumerate}

\subsection{Formulation}
X  is  sampled  from  a  probability  distribution  and  Y  is  sampled  from  a  probability  distribution conditioned on x respectively

Now, we try to estimate \sum_{x,y\in\ D}I(h(x) $\not=$ y) in terms of probability.

Using, Law of large numbers

[\sum_{x,y\in\ D}I(h(x) $\not=$ y)]/$|D|$ \approx E[I(h(x) $\not=$ y)]] = P(h(x) $\not=$ y)
\begin{itemize}
 
    
    
\end{itemize}
\section{Probabilistic Distributions}
  In the previous lecture were attempting to deduce a deterministic functional relationship between the sample data $\mathbf{x}$ and it's associated label $y$. However it is also possible, and much more probable, that the data is actually probabilistic in nature, due to the nature of errors, ambiguity, etc. \\
  Thus we need to translate our error-function minimization condition into a statement involving probabilities. \textcolor{red}{Expression in probabilities also allows for the assignment of a confidence value to the output of our algorithm.} 
  \begin{flushleft}
    Q. \textbf{How can the expression for error of the predictor function be written in a probabilistic manner?} \\
    \color{blue} Given that $\mathbf{x}$ and $y$ are sampled from a distribution with pdf $f_{XY}\left( \mathbf{x}, y \right)$,
    \begin{align*}
      \frac{1}{N_D} \sum_{(x,y) \in D} \mathbbm{1} \left( h(\mathbf{x}) \neq y \right) &\longrightarrow \mathbb{E}\left( \mathbbm{1} \left( h(\mathbf{x}) \neq y \right) \right)  \tag{Central Limit Theorem}\\
      &= \int_D \mathbbm{1} \left( h(\mathbf{x}) \neq y \right) f_{XY}\left( \mathbf{x}, y \right) d(\mathbf{x},y) \\
      &= \int_{ \left\{ h(\mathbf{x}) \neq y \right\} \subset D} \cancelto{1}{\mathbbm{1} \left( h(\mathbf{x}) \neq y \right)} f_{XY}\left( \mathbf{x}, y \right) d(\mathbf{x},y) \\
      & \hspace{1em} + \int_{ \left\{ h(\mathbf{x}) = y \right\} \subset D} \cancelto{0}{\mathbbm{1} \left( h(\mathbf{x}) \neq y \right)} f_{XY}\left( \mathbf{x}, y \right) d(\mathbf{x},y) \\
      &= \int_{ \left\{ h(\mathbf{x}) \neq y \right\} \subset D} f_{XY}\left( \mathbf{x}, y \right) d(\mathbf{x},y) \\
      \implies \text{Error}\left( h \right) &\simeq N_D \cdot \mathbb{P} \left( h(\mathbf{x}) \neq y \right)
    \end{align*}
  \end{flushleft}
    
    \subsection{$y$ Prediction}
    Since in most problems the instance of interest is known, $f_X$ is not of much interest. We instead aim to approximate the probability distribution of the label $y$ given $\mathbf{x}$. Since y is a binary random variable, it must have Bernoulli distribution. Assuming a sigmoid variation of the success probability, 
    \begin{align*}
      y & \sim \text{Bernoulli} \left( \frac{1}{1+e^{-\mathbf{w}^T \mathbf{x}}} \right) \\
      \implies \mathbb{P} \left( y =  1 \mid \mathbf{x} \right) &= \frac{1}{1+e^{-\mathbf{w}^T \mathbf{x}}} \\
      \implies \mathbb{P} \left( y = -1 \mid \mathbf{x} \right) &= 1 - \frac{1}{1+e^{-\mathbf{w}^T \mathbf{x}}} \\
      &= \frac{1}{1+e^{\mathbf{w}^T \mathbf{x}}} \\
      \implies \mathbb{P} \left( y \mid \mathbf{x} \right) &= \frac{1}{1 + e^{ -y \cdot \mathbf{w}^T \mathbf{x}}}
    \end{align*}
    Note that we have chosen a sigmoid form for $h$ since we need to trim the maximal space over which $h(x)$ exists, otherwise it is too large to conduct any meaningful search. The sigmoid function is good for this purpose becuase it's binary nature in the asymptotic limit. \\
    Thus with this probabilisitc nature of y, we can devise a maximum likelihood estimator for it, instead of a deterministic $h(x)$.



\subsection{Maximum likelihood estimator}
\normalfont
\begin{itemize}
  \item In general, we fix the function upto a parameter, $P(.|x)$=f($\theta$,x).
  \item Now we find $max_{\theta} \prod_{(x,y) \in D}P_{\theta}(y|x)$ and get theta which maximises $P(y|x)$ over the function set f($\theta$,x) and set of data points D.
  \item Given the distribution of a statistical
model f($\theta$;x) with unkown deterministic parameter $\theta$, MLE is to estimate the parameter $\theta$ by maximizing
the probability f($\theta$; x) with observations x
  \item Instead of product, we take logarithm, ie $max_{\theta} \sum_{(x,y) \in D}log(P_{\theta}(y|x))$ (log-likelihood estimate)
  \item The key properties of maximum likelihood estimator are its unbiasedness and minimum variance achieved over all estimators.
  \item That is $E(\hat{\theta_{i}})=\theta*$ 
  \item (We generate $\theta_{i}$ by sampling $(x,y)$ from D)
  \item Also this method minimises $E(|\hat{\theta}-\theta*|^2$ ie minimises the variance over all estimators.
  

\end{itemize} 
 
\subsection{Approximating function in general situation}
\begin{itemize}
  \item Underlying concept in deep learning is that there is a generic form of distribution.
  \item As mentioned in class, a linear-ReLU-linear can approximate any non linear function.
  \item But no such approximation in probability so we come up with a base distribution : Normal, Logistic depending on output.

\end{itemize}
\newcommand{\groupmember}[2] {
#1 ( \textbf{#2})
}
\section{Group Details and Individual Contribution}
\textbf{Group Members} 

\groupmember{Varad Mahashabde -}{200260057}
\\
Contribution:Formulation and Probabilistic Distributions, y-prediction
\\
\groupmember{Shirish Chinchanikar -}{19B090012}
\\
Contribution: Maximum Likelihood Estimators and Approximating functions in general situation
\groupmember{Prakriti Shetty -}{200020095}
\\
Contribution: Introduction, Surrogate functions in Deterministic Space
\\
\groupmember{Yuvraj Singh  -}{200070093}
\\
Contribution: Surrogate functions in Deterministic Space and Formulation
\\
\groupmember{Parth Vijay Dange -}{200020091}

 
\end{document}





